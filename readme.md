# simple nmt implementation with pytorch
everything is literally taken from [transformers without tears](https://github.com/tnq177/transformers_without_tears) with transformer replaced with lstm-based nmt. This implementation uses unidirectional LSTM encoder with residual connection from input embedding to output. For decoder, I use [feed input](https://arxiv.org/abs/1508.04025) along with [ScaleNorm](https://arxiv.org/pdf/1910.05895.pdf). [Fixnorm](https://arxiv.org/pdf/1710.01329.pdf) is also applied. This implementation combines several techniques such as label smoothing, ScaleNorm + FixNorm and altogether yields even better BLEU than my old one (implemented in TF, the difference in performance has nothing to do with TF -*peace*-). Previously with fixnorm I had to restrict the norm of embedding and final output vector to 5 each, but with label smoothing, ScaleNorm can freely learn its scale (see our [transformers without tears paper](https://arxiv.org/pdf/1910.05895.pdf)). I just ran a simple one and got ~28.5 compared to previous ~27.3. Anyway, this is only kept for reference cuz transformer is way better. I don't really use this code...
